"""
çŸ¥è¯†å›¾è°±æ£€ç´¢æµ‹è¯•è„šæœ¬ - é˜²æ´ªé¢„æ¡ˆ
æµ‹è¯• Phase 2 æ„å»ºçš„ Knowledge Graph æ•ˆæœ
"""
import os
import sys

# Windows encoding fix
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from pathlib import Path
from dotenv import load_dotenv
from llama_index.core import (
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.retrievers import KnowledgeGraphRAGRetriever
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Load environment
load_dotenv()
sys.path.insert(0, str(Path(__file__).parents[1]))  # Add project root

ROOT = Path(__file__).resolve().parents[1]
KG_DIR = str(ROOT / "src" / "db" / "knowledge_graph")

def test_kg_retrieval():
    print("="*80)
    print("  é˜²æ´ªé¢„æ¡ˆçŸ¥è¯†å›¾è°±æ£€ç´¢æµ‹è¯•")
    print("="*80)

    if not os.path.exists(KG_DIR):
        print(f"âŒ é”™è¯¯: çŸ¥è¯†å›¾è°±ç›®å½•ä¸å­˜åœ¨ {KG_DIR}ã€‚è¯·å…ˆè¿è¡Œ create_knowledge_graph.py")
        return

    # 1. Initialize Objects (Must match creation config)
    print("Initialize LLM & Embedding...")
    # Use OneKE to stay consistent with the extraction phase and VRAM usage
    llm = Ollama(
        model="oneke", 
        request_timeout=120.0,
        context_window=1024
    )
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-zh-v1.5", device="cpu")
    Settings.llm = llm
    Settings.embed_model = embed_model

    # 2. Load KG Index
    print(f"Loading Knowledge Graph from {KG_DIR}...")
    storage_context = StorageContext.from_defaults(persist_dir=KG_DIR)
    kg_index = load_index_from_storage(storage_context)
    print("âœ… Index Loaded Successfully!")

    # 3. Test Queries
    queries = [
        "å¸¸åº„æ°´åº“é˜²æ±›æŒ‡æŒ¥éƒ¨çš„æŒ‡æŒ¥é•¿æ˜¯è°ï¼Ÿ",
        "é˜²æ´ªé¢„æ¡ˆä¸­åŒ…å«å“ªäº›ç‰©èµ„ä¿éšœæªæ–½ï¼Ÿ",
        "è°è´Ÿè´£å ¤é˜²çš„å·¡æŸ¥å·¥ä½œï¼Ÿ",
        "å¯åŠ¨é˜²æ´ªé¢„æ¡ˆçš„æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    print("\n" + "="*60)
    print("å¼€å§‹æµ‹è¯•æ£€ç´¢...")
    print("="*60 + "\n")

    # Use Retriever (Retrieves relevant triplets/text)
    # æ–¹å¼ A: æ··åˆæ£€ç´¢ (Entity Matching + Vector)
    retriever = kg_index.as_retriever(
        include_text=True, # åŒ…å«åŸå§‹æ–‡æœ¬å—
        retriever_mode="hybrid", # æ··åˆæ£€ç´¢å®ä½“å’Œæ–‡æœ¬
        similarity_top_k=5
    )

    for q in queries:
        print(f"â“ é—®é¢˜: {q}")
        response = retriever.retrieve(q)
        
        print(f"ğŸ” æ£€ç´¢åˆ°çš„èŠ‚ç‚¹æ•°: {len(response)}")
        for i, node in enumerate(response[:3]): # Show top 3
            print(f"   [{i+1}] Score: {node.score:.3f}")
            # KG èŠ‚ç‚¹é€šå¸¸åŒ…å«ä¸‰å…ƒç»„ä¿¡æ¯å­—ç¬¦ä¸²
            content = node.text.replace('\n', ' ')[:150]
            print(f"       {content}...")
        print("-" * 50)
        
    print("\nâœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_kg_retrieval()
