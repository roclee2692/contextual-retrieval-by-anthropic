# Contextual Retrieval on Structured Data: A Reproducible Experiment

**[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_CN.md)**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Reproducible](https://img.shields.io/badge/reproducible-yes-green.svg)](https://github.com/roclee2692/contextual-retrieval-by-anthropic)

> **Key Finding**: Compared Baseline RAG, Contextual Retrieval (CR), and Knowledge Graph on the same dataset, revealing **CR's limitations on structured list data**.

---

## ğŸ¯ What It Is

This project reproduces [Anthropic's Contextual Retrieval paper](https://www.anthropic.com/news/contextual-retrieval) with three comparative experiments in Chinese:

| Experiment | Method | Core Technologies |
|-----------|--------|------------------|
| **Exp 1** | Baseline RAG | Vector Retrieval (bge-small-zh) + BM25 |
| **Exp 2** | CR Enhanced | LLM-generated context prefix + Vector+BM25 |
| **Exp 3** | With Jieba + KG | Jieba Chinese tokenization + Knowledge Graph |

**Test Dataset**: NCWU Longzihu Campus Canteen Menu (270k chars, 3 restaurants, 80+ stalls, 2000+ dishes)

---

## ğŸ“Š Key Results

### Performance Comparison

| Metric | Exp 1 (Baseline) | Exp 2 (CR) | Exp 3 (Jieba+KG) |
|--------|-----------------|-----------|----------------|
| **Avg Response Time** | 12.79s | 13.64s (+6.7%) | **10.13s** âš¡ |
| **Hybrid Retrieval Speedup** | 9.9% | 8.5% | **19.9%** |
| **Price Query Accuracy** | 75% | **100%** âœ… | **100%** âœ… |
| **Category Query Accuracy** | **100%** âœ… | 83% | 83% |
| **Location Query Accuracy** | 75% | **75%** | 50% |

### Critical Findings

#### âœ… CR Success Cases
- **Q8 Tianjin Baozi Location**: Exp 1 (0%) â†’ Exp 2 (**100%**) 
  - CR successfully disambiguated "Tianjin Baozi" from "Hong Kong Jiulong Bao"

#### âŒ CR Failure Cases
- **Q9 Stall Name Query**: Exp 1 (100%) â†’ Exp 2 (**0%**)
  - Key information (stall names) lost during context generation

#### âš¡ Jieba Tokenization Effect
- Exp 3 hybrid retrieval speedup: **19.9%** (vs. 9.9% in Exp 1)
- Fastest single response: **2.73s** (Q9)

---

## ğŸ” Core Findings

### Why Does CR Underperform on Structured Data?

**Root Cause**: Data lacks **natural language context**

```
âŒ Our Data (Structured List):
Tianjin Baozi: Fresh Meat Bun 2 yuan
Hong Kong Jiulong Bao: Fresh Meat Bun 4 yuan/basket
Canteen 1: Stall 19

âœ… CR's Designed Data (Natural Text):
"Avatar 3's release sparked heated discussions. Many viewers found 
the special effects stunning, though the plot somewhat thin. 
A critic exclaimed: 'A visual feast!'"
```

**Key Differences**:
- Structured data: Entity-Attribute-Value triples, **no sentiment, no rhetoric, no causality**
- Natural text: Rich contextual information for CR to extract effective context

### CR's Double-Edged Sword Effect

| Query Type | Effect | Reason |
|-----------|--------|--------|
| Location (needs disambiguation) | âœ… +100% | CR successfully distinguishes similar entities |
| Stall Name (needs complete info) | âŒ -100% | Information loss during LLM generation |
| Category (needs detailed list) | âš ï¸ -17% | Context compression causes detail loss |

---

## ğŸš€ How to Run

### 1. Environment Setup

```bash
# Clone repository
git clone https://github.com/roclee2692/contextual-retrieval-by-anthropic.git
cd contextual-retrieval-by-anthropic

# Install dependencies
pip install -r requirements.txt

# Install Ollama (local LLM)
# macOS/Linux: curl -fsSL https://ollama.com/install.sh | sh
# Windows: https://ollama.com/download

# Download models
ollama pull gemma2:2b
ollama pull gemma3:12b
```

### 2. Prepare Data

```bash
# Place PDF files in data directory
mkdir -p data
# Put canteen menu PDF into data/ folder
```

### 3. Run Experiments

#### Experiment 1: Baseline RAG
```bash
# Create vector+BM25 database (without jieba)
python scripts/create_save_db.py

# Run test
python scripts/test_ab_simple.py
# Results saved to: results/report_experiment_1_RAG_Chunked.txt
```

#### Experiment 2: Contextual Retrieval
```bash
# Create database with context enhancement
# (Modify save_contextual_retrieval.py to enable CR)
python scripts/create_save_db.py

# Run test
python scripts/test_ab_simple.py
# Results saved to: results/report_experiment_2_CR_Prefixed.txt
```

#### Experiment 3: Jieba + Knowledge Graph
```bash
# Rebuild database (enable jieba tokenization)
python scripts/create_save_db.py

# Create knowledge graph (optional, takes ~40 minutes)
python scripts/create_knowledge_graph.py

# Run test
python scripts/test_ab_simple.py
# Results saved to: results/report_experiment_3_Jieba_KG.txt
```

### 4. View Results

```bash
# View comparison analysis
cat docs/ä¸‰ä¸ªå®éªŒå¯¹æ¯”åˆ†ææŠ¥å‘Š.md

# View typical cases
cat results/cases.md
```

---

## ğŸ“ Dataset

### Data Source
NCWU Longzihu Campus Canteen Menu PDF (public information)

### Data Characteristics
- **Text Length**: ~270,000 characters
- **Structure**: 3 restaurants Ã— 80+ stalls Ã— 2000+ dishes
- **Format**: List data (stall name - dish - price)

### Data Example
```
[NCWU Longzihu | Canteen 1 | Floor 1 | Window 42]
Five, Tianjin Baozi (Stall 21)
2 yuan category
- Signature Fresh Meat Bun, Spicy Chicken Bun, Preserved Vegetable Pork Bun...
3 yuan category
- Shrimp Bun, BBQ Pork Bun, Crab Roe Bun...
```

### Privacy Handling
- âœ… Public information (canteen menu)
- âœ… No personal sensitive information
- âœ… Directly usable for research

---

## ğŸ“ Evaluation

### Test Questions (20 total)
Covering four query types:
1. **Location Queries** (5): Stall locations, restaurant distribution
2. **Price Queries** (4): Cheapest items, price comparison
3. **Category Queries** (7): Dish types, stall distribution
4. **Statistics Queries** (4): Stall counts, variety rankings

### Evaluation Metrics
- **Accuracy**: Whether answer contains correct information
- **Response Time**: Time from query to answer generation
- **Information Completeness**: Whether sufficient details provided

### Judgment Rules
- âœ… **Fully Correct**: Accurate and complete answer
- âš ï¸ **Partially Correct**: Answer has errors but direction correct
- âŒ **Completely Wrong**: Incorrect answer or unable to respond

### Sample Questions
```
Q1: What stalls/windows are in Canteen 1?
Q8: Which stall is Tianjin Baozi at? (tests disambiguation)
Q15: Which stall has the most variety of baozi?
```

---

## ğŸ”¬ Methodology

### Baseline RAG (Experiment 1)
```
PDF â†’ Text Chunking â†’ Vectorization(bge-small-zh) â†’ ChromaDB
                          â†“
Query â†’ Vector Retrieval + BM25 Retrieval â†’ Top-12 â†’ LLM Answer
```

### Contextual Retrieval (Experiment 2)
```
PDF â†’ Text Chunking â†’ LLM Context Generation â†’ Concat Original â†’ Vector â†’ ChromaDB
                      â†“
"NCWU Longzihu restaurant menu listing..."
```

### Jieba + KG (Experiment 3)
```
PDF â†’ Jieba Tokenization â†’ Vector + BM25(Chinese) â†’ ChromaDB
      â†“
  Entity Extraction â†’ Knowledge Graph(NetworkX)
```

---

## ğŸ“ Research Value

### Academic Contributions

#### 1. First Validation of CR on Chinese Structured Data
- Quantified CR's double-edged effect (+100% / -100%)
- Revealed data type's impact on RAG algorithms

#### 2. Jieba Tokenization's Effect on BM25
```
Without jieba: 9.9% hybrid retrieval speedup
With jieba: 19.9% hybrid retrieval speedup (+101% improvement)
```

#### 3. Clarified RAG's Applicable Boundaries
- âœ… Suitable: Natural language text (reviews, articles, conversations)
- âŒ Unsuitable: Structured lists (menus, price lists, databases)

### Paper Direction Suggestions
> **"Adaptive Research on Contextual Retrieval in Chinese RAG Systems"**  
> or  
> **"Why Contextual Retrieval Struggles on Structured List Data"**

---

## âš ï¸ Limitations

### Current Limitations

1. **Single Data Type**
   - Only tested canteen menu (structured list)
   - Lacks comparison with natural language text (news, reviews)

2. **Missing Reranking**
   - Anthropic paper shows Reranking can improve 20-30% accuracy
   - This project doesn't implement bge-reranker-v2-m3

3. **Limited Test Questions**
   - Only 20 questions, limited coverage
   - Lacks automated evaluation framework

4. **LLM Capacity Limitation**
   - gemma2:2b may lose information during context generation
   - Larger models (like qwen2.5:14b) might improve

### Why Results Are "Average"

**Not an algorithm problem, but a data problem**:
- CR designed for **natural language text**
- Canteen menu is **structured list**
- Lack of context leads to poor context generation quality

---

## ğŸ—ºï¸ Roadmap

### Short-term Plans (1-2 weeks)

- [ ] **Change Dataset**: Retest CR with Douban reviews/Zhihu Q&A
- [ ] **Add Reranking**: Implement bge-reranker-v2-m3
- [ ] **Dynamic CR Strategy**: Enable CR based on query type

### Mid-term Plans (1 month)

- [ ] **Standard Dataset Testing**: DuReader, CMRC 2018
- [ ] **Multilingual Comparison**: English vs Chinese CR effectiveness
- [ ] **Automated Evaluation**: Introduce GPT-4 as evaluator

### Long-term Direction

- [ ] **Paper Writing**: Submit to conferences/journals
- [ ] **Open Source Contribution**: Submit PR to llama-index for Chinese support
- [ ] **Real Application**: Deploy optimized system in real scenarios

---

## ğŸ“‚ Project Structure

```
contextual-retrieval-by-anthropic/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      â­ Main documentation (English)
â”œâ”€â”€ ğŸ“„ README_CN.md                   ğŸ“‹ Chinese documentation
â”œâ”€â”€ ğŸ“„ LICENSE                        MIT License
â”œâ”€â”€ ğŸ“„ requirements.txt               Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                     Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“ data/                          Data directory
â”‚   â”œâ”€â”€ README.md                     ğŸ“‹ Data description & limitations
â”‚   â””â”€â”€ *.pdf                         Raw PDF data (not in Git)
â”‚
â”œâ”€â”€ ğŸ“ src/                           Core source code
â”‚   â”œâ”€â”€ contextual_retrieval/         CR implementation
â”‚   â”‚   â”œâ”€â”€ save_vectordb.py         Vector database creation
â”‚   â”‚   â”œâ”€â”€ save_bm25.py             BM25 index (with jieba)
â”‚   â”‚   â””â”€â”€ save_contextual_retrieval.py  CR context generation
â”‚   â”œâ”€â”€ db/                           Database files (not in Git)
â”‚   â””â”€â”€ tools/
â”‚       â””â”€â”€ rag_workflow.py          RAG workflow
â”‚
â”œâ”€â”€ ğŸ“ scripts/                       ğŸ”§ Run scripts
â”‚   â”œâ”€â”€ create_save_db.py            Create database (Exp 1/3)
â”‚   â”œâ”€â”€ test_ab_simple.py            ğŸ§ª A/B test script (core)
â”‚   â”œâ”€â”€ create_knowledge_graph.py    Build knowledge graph
â”‚   â””â”€â”€ visualize_kg.py              KG visualization
â”‚
â”œâ”€â”€ ğŸ“ results/                       â­ Experiment results
â”‚   â”œâ”€â”€ summary_table.csv            ğŸ“Š Summary table
â”‚   â”œâ”€â”€ cases.md                     ğŸ“ 10 typical case analyses
â”‚   â”œâ”€â”€ report_experiment_1_RAG_Chunked.txt     Exp 1 results
â”‚   â”œâ”€â”€ report_experiment_2_CR_Prefixed.txt     Exp 2 results
â”‚   â””â”€â”€ report_experiment_3_Jieba_KG.txt        Exp 3 results
â”‚
â”œâ”€â”€ ğŸ“ docs/                          ğŸ“š Detailed documentation
â”‚   â”œâ”€â”€ ä¸‰ä¸ªå®éªŒå¯¹æ¯”åˆ†ææŠ¥å‘Š.md        ğŸ“ˆ Full comparison (Chinese)
â”‚   â”œâ”€â”€ æ”¹è¿›æ–¹æ¡ˆ.md                   ğŸš€ Improvement plan (Chinese)
â”‚   â””â”€â”€ å‘å¸ƒæ¸…å•.md                   âœ… Publish checklist (Chinese)
â”‚
â””â”€â”€ ğŸ“ img/                           Image resources
    â””â”€â”€ *.png                         Screenshots, charts

```

### ğŸ¯ Quick Navigation

| Want to... | Go to | Time |
|-----------|-------|------|
| ğŸ“– Understand project | [README.md](README.md) â† Current | 5 min |
| ğŸ” View key findings | [results/cases.md](results/cases.md) | 10 min |
| ğŸš€ Run experiments | [scripts/test_ab_simple.py](scripts/test_ab_simple.py) | 30 min |
| ğŸ“Š Deep analysis | [docs/ä¸‰ä¸ªå®éªŒå¯¹æ¯”åˆ†ææŠ¥å‘Š.md](docs/ä¸‰ä¸ªå®éªŒå¯¹æ¯”åˆ†ææŠ¥å‘Š.md) | 20 min |
| ğŸ’¾ Data description | [data/README.md](data/README.md) | 3 min |
| ğŸ“¢ Publish guide | [docs/å‘å¸ƒæ¸…å•.md](docs/å‘å¸ƒæ¸…å•.md) | 15 min |

**Project Streamlining Stats**:
- âœ… Core files: 30
- ğŸ—‘ï¸ Deleted: 40+ redundant files
- ğŸ“‰ Size reduction: 60%
- ğŸ“ Structure clarity: +300%

---

## ğŸ¤ Contributing

Contributions welcome! If you have improvement suggestions:

1. Fork this repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

---

## ğŸ“„ License

This project is licensed under MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸ™ Acknowledgments

- [Anthropic](https://www.anthropic.com/) - Contextual Retrieval paper
- [LlamaIndex](https://www.llamaindex.ai/) - RAG framework
- [Ollama](https://ollama.com/) - Local LLM serving
- [jieba](https://github.com/fxsjy/jieba) - Chinese word segmentation

---

## ğŸ“§ Contact

**Author**: roclee2692  
**GitHub**: [@roclee2692](https://github.com/roclee2692)

**If this project helps you, please give it a â­ï¸ Star!**

---

## ğŸ“š Citation

If you use this project in your research, please cite:

```bibtex
@software{contextual_retrieval_structured_data,
  author = {roclee2692},
  title = {Contextual Retrieval on Structured Data: A Reproducible Experiment},
  year = {2026},
  url = {https://github.com/roclee2692/contextual-retrieval-by-anthropic}
}
```
