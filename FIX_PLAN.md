# ğŸ”§ RAGç³»ç»Ÿä¿®å¤è®¡åˆ’

## ğŸ“‹ å½“å‰é—®é¢˜è¯Šæ–­

### ğŸ”´ å…³é”®é—®é¢˜
1. **BM25è¯„åˆ†å¼‚å¸¸** - æ‰€æœ‰ç›¸å…³æ€§åˆ†æ•°éƒ½æ˜¯0.0000
2. **æ£€ç´¢ç»“æœé”™è¯¯** - æŸ¥è¯¢"åŒ…å­"è¿”å›"éº»è¾£çƒ«"å’Œ"æ‹Œé¢"
3. **å¢å¼ºæ£€ç´¢è¡¨ç°å·®** - 5.5/10åˆ†ï¼Œä¸å¦‚åŸºå‡†æµ‹è¯•

### âš ï¸ å¯èƒ½åŸå› 
1. BM25ç´¢å¼•æ„å»ºé—®é¢˜
2. å‘é‡æ¨¡å‹ç»´åº¦ä¸ä¸€è‡´ï¼ˆ512 vs 768ï¼‰
3. æ—§æ•°æ®åº“æ®‹ç•™å¯¼è‡´å†²çª
4. åˆ†è¯å™¨åœ¨ä¿å­˜/åŠ è½½æ—¶ä¸¢å¤±

---

## ğŸ¯ ä¿®å¤è®¡åˆ’ï¼ˆåˆ†3é˜¶æ®µï¼‰

### é˜¶æ®µ1ï¸âƒ£: æ¸…ç†ä¸éªŒè¯ï¼ˆä¼˜å…ˆçº§ï¼šğŸ”¥ æé«˜ï¼‰

#### ä»»åŠ¡1.1: å®Œå…¨æ¸…ç†æ—§æ•°æ®åº“
```powershell
# åˆ é™¤æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶
Remove-Item -Recurse -Force ./src/db/canteen_db_bm25
Remove-Item -Recurse -Force ./src/db/canteen_db_vectordb
```

#### ä»»åŠ¡1.2: éªŒè¯æ•°æ®æº
- [ ] ç¡®è®¤ä½¿ç”¨ `CR_Prefixed_v2.pdf`
- [ ] æ£€æŸ¥PDFå†…å®¹æ˜¯å¦åŒ…å«"åŒ…å­"ã€"å¤©æ´¥åŒ…å­"ç­‰å…³é”®è¯
- [ ] ç»Ÿè®¡æ–‡æ¡£æ•°é‡åº”ä¸º232ä¸ª

#### ä»»åŠ¡1.3: æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬
```powershell
pip list | Select-String -Pattern "bm25|llama-index|jieba"
```
- [ ] bm25s: 0.2.14 âœ“
- [ ] llama-index-retrievers-bm25: 0.6.5 âœ“
- [ ] jieba: æœ€æ–°ç‰ˆæœ¬

---

### é˜¶æ®µ2ï¸âƒ£: ä¿®å¤BM25æ„å»ºï¼ˆä¼˜å…ˆçº§ï¼šğŸ”¥ æé«˜ï¼‰

#### ä»»åŠ¡2.1: æ£€æŸ¥BM25ä¿å­˜é€»è¾‘
**æ–‡ä»¶**: `src/contextual_retrieval/save_bm25.py`

**éœ€è¦éªŒè¯çš„ç‚¹**:
```python
# âœ“ åˆ†è¯å™¨å®šä¹‰æ˜¯å¦æ­£ç¡®
def chinese_tokenizer(text):
    tokens = list(jieba.cut_for_search(text))
    enhanced_tokens = []
    for token in tokens:
        enhanced_tokens.append(token)
        if 'åŒ…' in token:
            enhanced_tokens.append('åŒ…')
            enhanced_tokens.append('åŒ…å­')
    return enhanced_tokens

# âœ“ BM25åˆ›å»ºå‚æ•°
bm25_retriever = BM25Retriever.from_defaults(
    nodes=nodes,
    similarity_top_k=12,
    tokenizer=chinese_tokenizer,  # ç¡®ä¿ä¼ å…¥
)

# âœ“ ä¿å­˜æ–¹æ³•
bm25_retriever.persist(save_pth)
```

#### ä»»åŠ¡2.2: æµ‹è¯•BM25åˆ†è¯æ•ˆæœ
åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_bm25_tokenizer.py`:
```python
import jieba

def chinese_tokenizer(text):
    tokens = list(jieba.cut_for_search(text))
    enhanced_tokens = []
    for token in tokens:
        enhanced_tokens.append(token)
        if 'åŒ…' in token:
            enhanced_tokens.append('åŒ…')
            enhanced_tokens.append('åŒ…å­')
    return enhanced_tokens

# æµ‹è¯•ç”¨ä¾‹
test_texts = [
    "å¤©æ´¥åŒ…å­",
    "æˆ‘çˆ±æˆ‘ç²¥",
    "åŒ…å­ç±»é£Ÿå“",
    "å“ªé‡Œæœ‰åŒ…å­ï¼Ÿ"
]

for text in test_texts:
    tokens = chinese_tokenizer(text)
    print(f"'{text}' -> {tokens}")
```

#### ä»»åŠ¡2.3: ä¿®å¤BM25åŠ è½½é€»è¾‘
**é—®é¢˜**: åŠ è½½æ—¶å¯èƒ½æ²¡æœ‰æ­£ç¡®æ¢å¤åˆ†è¯å™¨

**æ£€æŸ¥**: `test_ab_simple.py` ä¸­çš„åŠ è½½ä»£ç 
```python
# å½“å‰ä»£ç ï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
self.bm25_retriever = BM25Retriever.from_persist_dir(
    self.bm25_db_path
)

# å¯èƒ½éœ€è¦æ”¹ä¸ºï¼ˆå¾…éªŒè¯ï¼‰
# åˆ†è¯å™¨å¯èƒ½éœ€è¦åœ¨åŠ è½½åé‡æ–°è®¾ç½®ï¼Ÿ
```

---

### é˜¶æ®µ3ï¸âƒ£: ç»Ÿä¸€å‘é‡æ¨¡å‹ï¼ˆä¼˜å…ˆçº§ï¼šğŸ”¥ é«˜ï¼‰

#### ä»»åŠ¡3.1: ç¡®è®¤å‘é‡æ¨¡å‹é…ç½®
**æ–‡ä»¶**: `create_save_db.py` æˆ–é…ç½®æ–‡ä»¶

**æ£€æŸ¥ç‚¹**:
```python
# ç¡®ä¿ä½¿ç”¨ä¸€è‡´çš„æ¨¡å‹
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-zh-v1.5",  # 512ç»´
    # æˆ–
    # model_name="BAAI/bge-base-zh-v1.5",  # 768ç»´
)
```

#### ä»»åŠ¡3.2: åˆ é™¤å¹¶é‡å»ºå‘é‡æ•°æ®åº“
```python
# ç¡®ä¿å®Œå…¨é‡å»º
import shutil
shutil.rmtree("./src/db/canteen_db_vectordb", ignore_errors=True)
shutil.rmtree("./src/db/canteen_db_bm25", ignore_errors=True)
```

---

### é˜¶æ®µ4ï¸âƒ£: é‡å»ºä¸æµ‹è¯•ï¼ˆä¼˜å…ˆçº§ï¼šğŸ”¥ æé«˜ï¼‰

#### ä»»åŠ¡4.1: é‡æ–°åˆ›å»ºæ•°æ®åº“
```powershell
# 1. åˆ é™¤æ—§æ•°æ®åº“
Remove-Item -Recurse -Force ./src/db/canteen_db_bm25 -ErrorAction SilentlyContinue
Remove-Item -Recurse -Force ./src/db/canteen_db_vectordb -ErrorAction SilentlyContinue

# 2. é‡æ–°åˆ›å»º
python create_save_db.py
```

#### ä»»åŠ¡4.2: éªŒè¯BM25è¯„åˆ†
åˆ›å»º `verify_bm25_scores.py`:
```python
from llama_index.retrievers.bm25 import BM25Retriever
import jieba

# å®šä¹‰åˆ†è¯å™¨
def chinese_tokenizer(text):
    tokens = list(jieba.cut_for_search(text))
    enhanced_tokens = []
    for token in tokens:
        enhanced_tokens.append(token)
        if 'åŒ…' in token:
            enhanced_tokens.append('åŒ…')
            enhanced_tokens.append('åŒ…å­')
    return enhanced_tokens

# åŠ è½½BM25
bm25_retriever = BM25Retriever.from_persist_dir(
    "./src/db/canteen_db_bm25"
)

# æµ‹è¯•æŸ¥è¯¢
test_queries = [
    "åŒ…å­",
    "å¤©æ´¥åŒ…å­",
    "æˆ‘çˆ±æˆ‘ç²¥",
    "å“ªäº›çª—å£æä¾›åŒ…å­"
]

for query in test_queries:
    print(f"\næŸ¥è¯¢: {query}")
    results = bm25_retriever.retrieve(query)
    
    for i, node in enumerate(results[:3], 1):
        score = node.score if hasattr(node, 'score') else 'N/A'
        text_preview = node.text[:100]
        print(f"  {i}. è¯„åˆ†: {score:.4f} | å†…å®¹: {text_preview}...")
        
    # æ£€æŸ¥è¯„åˆ†æ˜¯å¦éƒ½æ˜¯0
    scores = [n.score for n in results if hasattr(n, 'score')]
    if all(s == 0.0 for s in scores):
        print("  âš ï¸ è­¦å‘Š: æ‰€æœ‰è¯„åˆ†éƒ½æ˜¯0.0000ï¼")
```

#### ä»»åŠ¡4.3: è¿è¡Œå®Œæ•´æµ‹è¯•
```powershell
# è¿è¡ŒA/Bæµ‹è¯•
python test_ab_simple.py
```

---

## ğŸ” è¯Šæ–­æ£€æŸ¥æ¸…å•

### åœ¨é‡å»ºæ•°æ®åº“å‰
- [ ] ç¡®è®¤PDFæ–‡ä»¶è·¯å¾„æ­£ç¡®
- [ ] ç¡®è®¤å‘é‡æ¨¡å‹åç§°ä¸€è‡´
- [ ] ç¡®è®¤åˆ†è¯å™¨ä»£ç æ­£ç¡®
- [ ] åˆ é™¤æ‰€æœ‰æ—§æ•°æ®åº“æ–‡ä»¶

### åœ¨é‡å»ºæ•°æ®åº“å
- [ ] æ£€æŸ¥æ–‡æ¡£æ•°é‡ï¼ˆåº”ä¸º232ï¼‰
- [ ] éªŒè¯BM25è¯„åˆ†ä¸å…¨ä¸º0
- [ ] æµ‹è¯•"åŒ…å­"æŸ¥è¯¢è¿”å›æ­£ç¡®ç»“æœ
- [ ] è¿è¡Œå®Œæ•´20é—®é¢˜æµ‹è¯•

### åœ¨æµ‹è¯•å®Œæˆå
- [ ] å¯¹æ¯”æ–°æ—§æµ‹è¯•æŠ¥å‘Š
- [ ] è®°å½•æ€§èƒ½æŒ‡æ ‡
- [ ] ç¡®è®¤åŒ…å­ç±»æŸ¥è¯¢æ˜¯å¦ä¿®å¤

---

## ğŸ“Š é¢„æœŸç»“æœ

### ğŸ¯ ä¿®å¤ç›®æ ‡
1. **BM25è¯„åˆ†æ­£å¸¸**: åˆ†æ•°èŒƒå›´åº”åœ¨0.1-10.0ä¹‹é—´
2. **åŒ…å­ç±»æŸ¥è¯¢æˆåŠŸ**: Q3, Q7, Q8, Q15 å…¨éƒ¨æ­£ç¡®
3. **ç»¼åˆæ€§èƒ½æå‡**: å¢å¼ºæ£€ç´¢åˆ†æ•°ä»5.5 â†’ 7.5+

### ğŸ“ˆ æˆåŠŸæ ‡å‡†
| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | éªŒè¯æ–¹æ³• |
|------|------|------|---------|
| BM25éé›¶è¯„åˆ†ç‡ | 0% | 100% | verify_bm25_scores.py |
| åŒ…å­æŸ¥è¯¢å‡†ç¡®ç‡ | 0% (0/4) | 100% (4/4) | test_ab_simple.py Q3,7,8,15 |
| å¢å¼ºæ£€ç´¢æ€»è¯„åˆ† | 5.5/10 | â‰¥7.5/10 | å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š |
| å¹³å‡å“åº”æ—¶é—´ | 14.79s | <10s | test_ab_simple.py |

---

## ğŸš€ æ‰§è¡Œæ­¥éª¤ï¼ˆæŒ‰é¡ºåºï¼‰

### Step 1: ç«‹å³æ‰§è¡Œï¼ˆ5åˆ†é’Ÿï¼‰
```powershell
# 1.1 åˆ›å»ºBM25åˆ†è¯æµ‹è¯•
python -c "
import jieba

def chinese_tokenizer(text):
    tokens = list(jieba.cut_for_search(text))
    enhanced_tokens = []
    for token in tokens:
        enhanced_tokens.append(token)
        if 'åŒ…' in token:
            enhanced_tokens.append('åŒ…')
            enhanced_tokens.append('åŒ…å­')
    return enhanced_tokens

test_cases = ['å¤©æ´¥åŒ…å­', 'æˆ‘çˆ±æˆ‘ç²¥', 'åŒ…å­ç±»é£Ÿå“']
for text in test_cases:
    print(f'{text} -> {chinese_tokenizer(text)}')
"

# 1.2 æ£€æŸ¥å½“å‰æ•°æ®åº“å¤§å°
Get-ChildItem ./src/db/canteen_db_bm25 -Recurse | Measure-Object -Property Length -Sum
Get-ChildItem ./src/db/canteen_db_vectordb -Recurse | Measure-Object -Property Length -Sum
```

### Step 2: æ¸…ç†é‡å»ºï¼ˆ10åˆ†é’Ÿï¼‰
```powershell
# 2.1 å®Œå…¨åˆ é™¤æ—§æ•°æ®åº“
Remove-Item -Recurse -Force ./src/db/canteen_db_bm25 -ErrorAction SilentlyContinue
Remove-Item -Recurse -Force ./src/db/canteen_db_vectordb -ErrorAction SilentlyContinue

# 2.2 é‡æ–°åˆ›å»º
python create_save_db.py

# 2.3 éªŒè¯åˆ›å»ºç»“æœ
python quick_check.py
```

### Step 3: éªŒè¯ä¿®å¤ï¼ˆ15åˆ†é’Ÿï¼‰
```powershell
# 3.1 åˆ›å»ºå¹¶è¿è¡ŒBM25è¯„åˆ†éªŒè¯è„šæœ¬ï¼ˆè§ä»»åŠ¡4.2ï¼‰
# 3.2 è¿è¡Œå®Œæ•´æµ‹è¯•
python test_ab_simple.py

# 3.3 å¯¹æ¯”æŠ¥å‘Š
# æ¯”è¾ƒæ–°ç”Ÿæˆçš„æŠ¥å‘Šä¸ ab_test_report_20260114_182109.txt
```

---

## ğŸ“ åç»­ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### å¦‚æœä¿®å¤æˆåŠŸ
1. è°ƒæ•´BM25æƒé‡å‚æ•°
2. ä¼˜åŒ–æ··åˆæ£€ç´¢æ¯”ä¾‹
3. å¢åŠ ç¼“å­˜æœºåˆ¶

### å¦‚æœä»æœ‰é—®é¢˜
1. æ£€æŸ¥llama-indexç‰ˆæœ¬å…¼å®¹æ€§
2. å°è¯•ä¸åŒçš„åˆ†è¯å™¨ç­–ç•¥
3. è€ƒè™‘ä½¿ç”¨è‡ªå®šä¹‰BM25å®ç°

---

## ğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®

**ç«‹å³æ‰§è¡Œ**: 
1. è¿è¡Œ Step 1 è¯Šæ–­è„šæœ¬ï¼Œç¡®è®¤åˆ†è¯å™¨å·¥ä½œæ­£å¸¸
2. å¤‡ä»½å½“å‰æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦å›é€€ï¼‰
3. æ‰§è¡Œ Step 2 å®Œå…¨é‡å»ºæ•°æ®åº“
4. è¿è¡Œ Step 3 éªŒè¯ä¿®å¤æ•ˆæœ

**ç­‰å¾…åé¦ˆ**:
- å‘Šè¯‰æˆ‘ Step 1 çš„è¾“å‡ºç»“æœ
- æˆ‘ä¼šæ ¹æ®ç»“æœè°ƒæ•´åç»­æ­¥éª¤
