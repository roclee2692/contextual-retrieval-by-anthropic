"""
Phase 2 ä¸‰ç»„å®éªŒå®Œæ•´å¯¹æ¯”
- Baseline: åŸå§‹æ–‡æ¡£ç›´æ¥æ£€ç´¢ï¼ˆä¸é¢„æ„å»ºæ•°æ®åº“ï¼‰
- CR: ä¸Šä¸‹æ–‡å¢å¼ºæ£€ç´¢ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„CRæ•°æ®åº“ï¼‰
- KG: çŸ¥è¯†å›¾è°±æ¨ç†æ£€ç´¢ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„KGï¼‰
"""
import os
import sys
import json
import time
from pathlib import Path

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from dotenv import load_dotenv
sys.path.insert(0, str(Path(__file__).parents[1]))

from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings,
    QueryBundle
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore
from llama_index.llms.ollama import Ollama
import chromadb
import jieba
from typing import List

load_dotenv()

# æµ‹è¯•é—®é¢˜é›†ï¼ˆRedesigned for Phase 3: Attribute & Topology Testï¼‰
TEST_QUERIES = [
    # 1. æ•°å€¼å±æ€§ç±» - KG (å¦‚æœ‰Schemaä¼˜åŒ–) åº”æœ‰æå‡
    {"query": "æ¨å®¶æ¨ªæ°´åº“çš„æ±›é™æ°´ä½æ˜¯å¤šå°‘ï¼Ÿ", "category": "æ•°å€¼å±æ€§", "type": "äº‹å®æŸ¥è¯"},
    {"query": "æ³¼æ²³æ°´åº“çš„æ±›é™æ°´ä½æ˜¯å¤šå°‘ï¼Ÿ", "category": "æ•°å€¼å±æ€§", "type": "äº‹å®æŸ¥è¯"},
    
    # 2. è´£ä»»äººç±» - KG å¼ºé¡¹ (Topological)
    {"query": "æ¨å®¶æ¨ªæ°´åº“çš„å¤§åå®‰å…¨è´£ä»»äººæ˜¯è°ï¼Ÿ", "category": "å®ä½“å…³ç³»", "type": "è´£ä»»äººæŸ¥è¯¢"},
    {"query": "è°è´Ÿè´£é˜²æ´ªæŒ‡æŒ¥éƒ¨çš„ç»Ÿä¸€è°ƒåº¦ï¼Ÿ", "category": "å®ä½“å…³ç³»", "type": "èŒè´£æŸ¥è¯¢"},

    # 3. é€»è¾‘è§¦å‘ç±» - æ··åˆé¢†åŸŸ (Condition)
    {"query": "æ°´ä½è¶…è¿‡å¤šå°‘ç±³éœ€è¦å¯åŠ¨IIIçº§å“åº”ï¼Ÿ", "category": "é€»è¾‘æ¡ä»¶", "type": "æ¡ä»¶åˆ¤æ–­"},

    # 4. åˆ—è¡¨æšä¸¾ç±» - CR/Baseline å¼ºé¡¹
    {"query": "é˜²æ´ªæŠ¢é™©ç‰©èµ„å‚¨å¤‡åŒ…æ‹¬å“ªäº›ä¸œè¥¿ï¼Ÿ", "category": "æ¸…å•æšä¸¾", "type": "åˆ—è¡¨æŸ¥è¯¢"},
    
    # 5. å¤æ‚æ¨ç†ç±» - å¤šè·³
    {"query": "å¦‚æœä¸è¿›è¡Œç”šè‡³æ³„æ´ªï¼Œä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿ", "category": "å› æœæ¨ç†", "type": "æ¨ç†åˆ†æ"},

    # 6. é•¿æ–‡æœ¬æè¿°ç±» - Baseline/CR ç»å¯¹å¼ºé¡¹
    {"query": "è¯·è¯¦ç»†æè¿°å ¤é˜²å·¡æŸ¥çš„å…·ä½“æ­¥éª¤å’Œæ ‡å‡†ã€‚", "category": "é•¿æ–‡æè¿°", "type": "è§„åˆ™è¯´æ˜"}
]

def chinese_tokenizer(text):
    """ä¸­æ–‡åˆ†è¯å™¨"""
    return list(jieba.cut_for_search(text))

class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡+BM25ï¼‰"""
    def __init__(self, vector_retriever, bm25_retriever):
        self.vector_retriever = vector_retriever
        self.bm25_retriever = bm25_retriever
        super().__init__()

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        vector_results = self.vector_retriever.retrieve(query_bundle) if self.vector_retriever else []
        bm25_results = self.bm25_retriever.retrieve(query_bundle) if self.bm25_retriever else []
        
        all_nodes = {}
        for node in vector_results + bm25_results:
            if node.node_id not in all_nodes:
                all_nodes[node.node_id] = node
            else:
                # åˆå¹¶åˆ†æ•°ï¼ˆç®€å•å¹³å‡ï¼‰
                all_nodes[node.node_id].score = (all_nodes[node.node_id].score + node.score) / 2
        
        sorted_nodes = sorted(all_nodes.values(), key=lambda x: x.score if x.score else 0, reverse=True)
        return sorted_nodes[:10]

def init_baseline_retriever(db_path, bm25_path, collection_name):
    """åˆå§‹åŒ–Baselineæ£€ç´¢å™¨ - ä»é¢„æ„å»ºçš„æ•°æ®åº“åŠ è½½ï¼ˆä¸CRé‡‡ç”¨ç›¸åŒæ¶æ„ï¼‰"""
    print("ğŸ”¹ Baseline: ä»é¢„æ„å»ºæ•°æ®åº“åŠ è½½...")
    
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-zh-v1.5", device="cpu")
    
    if not os.path.exists(db_path):
        print(f"   âŒ æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        return None
    
    db = chromadb.PersistentClient(path=db_path)
    try:
        chroma_collection = db.get_collection(collection_name)
    except:
        print(f"   âŒ Collection ä¸å­˜åœ¨: {collection_name}")
        return None
    
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    vector_index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
    vector_retriever = vector_index.as_retriever(similarity_top_k=5)
    
    bm25_retriever = None
    if os.path.exists(bm25_path):
        print(f"   Using Jieba tokenizer for Baseline BM25 at {bm25_path}")
        # Load without tokenizer arg (avoids bm25s error)
        bm25_retriever = BM25Retriever.from_persist_dir(bm25_path)
        # Manually inject tokenizer for Query processing
        bm25_retriever._tokenizer = chinese_tokenizer
        bm25_retriever._similarity_top_k = 5
    
    return HybridRetriever(vector_retriever, bm25_retriever)

def init_cr_retriever(db_path, bm25_path, collection_name):
    """åˆå§‹åŒ–CRæ£€ç´¢å™¨ - ä»é¢„æ„å»ºçš„æ•°æ®åº“åŠ è½½"""
    print("ğŸ”¹ CR Enhanced: ä»é¢„æ„å»ºæ•°æ®åº“åŠ è½½...")
    
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-zh-v1.5", device="cpu")
    
    if not os.path.exists(db_path):
        print(f"   âŒ æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        return None
    
    db = chromadb.PersistentClient(path=db_path)
    try:
        chroma_collection = db.get_collection(collection_name)
    except:
        print(f"   âŒ Collection ä¸å­˜åœ¨: {collection_name}")
        return None
    
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    vector_index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
    vector_retriever = vector_index.as_retriever(similarity_top_k=5)
    
    bm25_retriever = None
    if os.path.exists(bm25_path):
        print(f"   Using Jieba tokenizer for CR BM25 at {bm25_path}")
        # Load without tokenizer arg (avoids bm25s error)
        bm25_retriever = BM25Retriever.from_persist_dir(bm25_path)
        # Manually inject tokenizer for Query processing
        bm25_retriever._tokenizer = chinese_tokenizer
        bm25_retriever._similarity_top_k = 5
    
    return HybridRetriever(vector_retriever, bm25_retriever)

def init_kg_retriever(kg_dir):
    """åˆå§‹åŒ–KGæ£€ç´¢å™¨"""
    print("ğŸ”¹ Knowledge Graph: ä»é¢„æ„å»ºå›¾è°±åŠ è½½...")
    
    if not os.path.exists(kg_dir):
        print(f"   âŒ KGç›®å½•ä¸å­˜åœ¨: {kg_dir}")
        return None
    
    # Use OneKE for consistency and VRAM constraints
    llm = Ollama(
        model="oneke", 
        request_timeout=120.0,
        context_window=1024
    )
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-zh-v1.5", device="cpu")
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    storage_context = StorageContext.from_defaults(persist_dir=kg_dir)
    kg_index = load_index_from_storage(storage_context)
    
    # ä½¿ç”¨æ··åˆæ¨¡å¼æ£€ç´¢ï¼ˆå®ä½“åŒ¹é…+å‘é‡ï¼‰
    retriever = kg_index.as_retriever(
        include_text=True,
        retriever_mode="hybrid",
        similarity_top_k=5
    )
    
    return retriever

def run_single_experiment(experiment_name, retriever, queries):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    print(f"\n{'='*70}")
    print(f"å®éªŒ: {experiment_name}")
    print(f"{'='*70}\n")
    
    if not retriever:
        print(f"âŒ {experiment_name} æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥")
        return None
    
    results = []
    for item in queries:
        query = item["query"]
        print(f"ğŸ” {query}")
        
        try:
            start = time.time()
            nodes = retriever.retrieve(QueryBundle(query))
            elapsed = time.time() - start
            
            if nodes and len(nodes) > 0:
                top_text = nodes[0].text[:150].replace('\n', ' ')
                top_score = nodes[0].score if nodes[0].score else 0
                results_count = len(nodes)
            else:
                top_text = "æ— ç»“æœ"
                top_score = 0
                results_count = 0
            
            result = {
                "query": query,
                "category": item["category"],
                "type": item["type"],
                "time": elapsed,
                "top_1_text": top_text,
                "top_1_score": top_score,
                "results_count": results_count
            }
            results.append(result)
            
            print(f"   â±ï¸  è€—æ—¶: {elapsed:.2f}s | å¾—åˆ†: {top_score:.3f} | ç»“æœæ•°: {results_count}")
            print(f"   ğŸ“„ {top_text}...\n")
            
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}\n")
            results.append({
                "query": query,
                "category": item["category"],
                "type": item["type"],
                "error": str(e)
            })
    
    return results

def generate_markdown_report(all_results, queries):
    """ç”ŸæˆMarkdownå¯¹æ¯”æŠ¥å‘Š"""
    report_path = Path("results/phase2_complete_comparison.md")
    
    md = "# Phase 2: å®Œæ•´ä¸‰ç»„å®éªŒå¯¹æ¯”åˆ†æ\n\n"
    md += f"**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    md += "## å®éªŒé…ç½®\n\n"
    md += "| å®éªŒç»„ | è¯´æ˜ | æ•°æ®æ¥æº |\n"
    md += "|--------|------|----------|\n"
    md += "| **Baseline** | åŸå§‹æ–‡æ¡£ç›´æ¥æ£€ç´¢ï¼ˆå‘é‡+BM25ï¼‰ | **é¢„æ„å»ºChromaDB** (ä¸CRç›¸åŒæ¶æ„) |\n"
    md += "| **CR Enhanced** | ä¸Šä¸‹æ–‡å¢å¼ºæ£€ç´¢ | **é¢„æ„å»ºChromaDB**ï¼Œå«LLMç”Ÿæˆçš„ä¸Šä¸‹æ–‡ |\n"
    md += "| **Knowledge Graph** | çŸ¥è¯†å›¾è°±æ¨ç†æ£€ç´¢ | é¢„æ„å»ºKGï¼Œä¸‰å…ƒç»„+å®ä½“å…³ç³» |\n\n"
    
    md += "## æµ‹è¯•é—®é¢˜åˆ†ç±»\n\n"
    md += "| ç±»å‹ | æ•°é‡ | è¯´æ˜ |\n"
    md += "|------|------|------|\n"
    types = {}
    for q in queries:
        t = q["type"]
        types[t] = types.get(t, 0) + 1
    for t, count in types.items():
        md += f"| {t} | {count} | - |\n"
    md += "\n"
    
    md += "## é€é¢˜è¯¦ç»†å¯¹æ¯”\n\n"
    
    for i, q_item in enumerate(queries):
        query = q_item["query"]
        md += f"### Q{i+1}: {query}\n\n"
        md += f"**ç±»å‹**: {q_item['type']} | **åˆ†ç±»**: {q_item['category']}\n\n"
        
        md += "| å®éªŒ | Top-1 å¾—åˆ† | è€—æ—¶(s) | Top-1 é¢„è§ˆ |\n"
        md += "|------|-----------|---------|------------|\n"
        
        for exp_name in ["Baseline", "CR_Enhanced", "KG"]:
            if exp_name in all_results:
                result = all_results[exp_name][i]
                score = result.get("top_1_score", 0)
                elapsed = result.get("time", 0)
                preview = result.get("top_1_text", "N/A")[:80].replace('|', '\\|')
                md += f"| {exp_name} | {score:.3f} | {elapsed:.2f} | {preview}... |\n"
        
        md += "\n"
    
    md += "## æ€§èƒ½ç»Ÿè®¡\n\n"
    md += "| å®éªŒ | å¹³å‡è€—æ—¶(s) | å¹³å‡å¾—åˆ† | æ— ç»“æœæ•° |\n"
    md += "|------|------------|----------|----------|\n"
    
    for exp_name, results in all_results.items():
        times = [r.get("time", 0) for r in results if "error" not in r]
        scores = [r.get("top_1_score", 0) for r in results if "error" not in r]
        no_results = sum(1 for r in results if r.get("results_count", 0) == 0)
        
        avg_time = sum(times) / len(times) if times else 0
        avg_score = sum(scores) / len(scores) if scores else 0
        
        md += f"| {exp_name} | {avg_time:.2f} | {avg_score:.3f} | {no_results} |\n"
    
    md += "\n## ç»“è®º\n\n"
    md += "_å¾…è¡¥å……ï¼šåŸºäºä¸Šè¿°æ•°æ®çš„å®šæ€§åˆ†æ_\n\n"
    
    report_path.write_text(md, encoding='utf-8')
    print(f"\nğŸ“Š å®Œæ•´å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    print("="*80)
    print("  Phase 2: ä¸‰ç»„å®éªŒå®Œæ•´å¯¹æ¯”")
    print("="*80)
    
    # é…ç½®
    DATA_DIR = os.getenv("DATA_DIR", "./data/é˜²æ´ªé¢„æ¡ˆ")
    
    # Baseline: ä½¿ç”¨é¢„æ„å»ºçš„æ•°æ®åº“ï¼ˆä¸CRç›¸åŒæ¶æ„ï¼‰
    BASELINE_VECTOR_DB = "./src/db/flood_prevention_db_baseline_vectordb"
    BASELINE_BM25_DB = "./src/db/flood_prevention_db_baseline_bm25"
    
    # CR Enhanced
    CR_VECTOR_DB = "./src/db/flood_prevention_db_cr_vectordb"
    CR_BM25_DB = "./src/db/flood_prevention_db_cr_bm25"
    
    # Knowledge Graph
    KG_DIR = "./src/db/knowledge_graph"
    
    COLLECTION_NAME = "flood_prevention_collection"
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    baseline_retriever = init_baseline_retriever(BASELINE_VECTOR_DB, BASELINE_BM25_DB, COLLECTION_NAME)
    cr_retriever = init_cr_retriever(CR_VECTOR_DB, CR_BM25_DB, COLLECTION_NAME)
    kg_retriever = init_kg_retriever(KG_DIR)
    
    all_results = {}
    
    # è¿è¡Œå®éªŒ
    if baseline_retriever:
        all_results["Baseline"] = run_single_experiment("Baseline", baseline_retriever, TEST_QUERIES)
    
    if cr_retriever:
        all_results["CR_Enhanced"] = run_single_experiment("CR Enhanced", cr_retriever, TEST_QUERIES)
    
    if kg_retriever:
        all_results["KG"] = run_single_experiment("Knowledge Graph", kg_retriever, TEST_QUERIES)
    
    # ç”ŸæˆæŠ¥å‘Š
    if all_results:
        generate_markdown_report(all_results, TEST_QUERIES)
    
    print("\nâœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")

if __name__ == "__main__":
    main()
