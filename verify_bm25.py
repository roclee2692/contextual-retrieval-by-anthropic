"""éªŒè¯BM25æ£€ç´¢æ•ˆæœ"""
from llama_index.retrievers.bm25 import BM25Retriever
import jieba
import bm25s

# å®šä¹‰ä¸­æ–‡åˆ†è¯å™¨ï¼ˆä¸åˆ›å»ºæ—¶ä¸€è‡´ï¼‰
def chinese_tokenizer(text):
    """å¢å¼ºå‹ä¸­æ–‡åˆ†è¯å™¨"""
    tokens = list(jieba.cut_for_search(text))
    enhanced_tokens = []
    for token in tokens:
        enhanced_tokens.append(token)
        if 'åŒ…' in token:
            enhanced_tokens.append('åŒ…')
            enhanced_tokens.append('åŒ…å­')
    return enhanced_tokens

# ğŸ”§ å…³é”®ä¿®å¤ï¼šæ›¿æ¢bm25s.tokenizeå‡½æ•°
original_tokenize = bm25s.tokenize

def patched_tokenize(text, *args, **kwargs):
    """ä½¿ç”¨æˆ‘ä»¬çš„ä¸­æ–‡åˆ†è¯å™¨"""
    if isinstance(text, str):
        return [chinese_tokenizer(text)]
    else:
        return [chinese_tokenizer(t) for t in text]

bm25s.tokenize = patched_tokenize

print("=== åŠ è½½BM25æ£€ç´¢å™¨ ===")
bm25_retriever = BM25Retriever.from_persist_dir(
    "./src/db/canteen_db_bm25"
)
print("âœ“ å·²æ›¿æ¢bm25s.tokenizeä¸ºä¸­æ–‡åˆ†è¯å™¨")

print("\n=== æµ‹è¯•å…³é”®æŸ¥è¯¢ ===\n")
test_queries = [
    "åŒ…å­",
    "å¤©æ´¥åŒ…å­", 
    "æˆ‘çˆ±æˆ‘ç²¥",
    "å“ªäº›çª—å£æä¾›åŒ…å­"
]

for query in test_queries:
    print(f"æŸ¥è¯¢: {query}")
    results = bm25_retriever.retrieve(query)
    
    for i, node in enumerate(results[:3], 1):
        score = node.score if hasattr(node, 'score') else 'N/A'
        text_preview = node.text[:80].replace('\n', ' ')
        print(f"  {i}. è¯„åˆ†: {score:8.4f} | {text_preview}...")
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯„åˆ†éƒ½æ˜¯0
    scores = [n.score for n in results if hasattr(n, 'score')]
    if all(s == 0.0 for s in scores):
        print("  âš ï¸ è­¦å‘Š: æ‰€æœ‰è¯„åˆ†éƒ½æ˜¯0.0000ï¼")
    
    print()
