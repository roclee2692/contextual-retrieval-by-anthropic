# 中文垂直领域检索增强方法的系统对比研究：上下文检索、知识图谱与重排序在水利防洪预案中的实证分析

---

## 摘要

面向水利防洪预案等结构化公文在应急场景中的条款定位与知识查询需求，本文构建并评测了"基线检索—上下文增强检索（Contextual Retrieval, CR）—Cross-Encoder 重排序"的检索式问答流程。针对中文垂直语料与本地小模型生成上下文的效果差异，本文设计了 2×2 消融实验（Baseline/CR × 无/有重排序），并在真实防洪预案语料与问题集（n=30）上进行对比。实验结果表明：在无重排序条件下，CR 相对 Baseline 显示有限改进（检索准确率由基线的 76.7% 提升至 80.0%，p < 0.001），但改进不均衡——数值查询提升 10%，实体和流程查询无改进，反映小模型上下文生成的质量与查询类型密切相关；引入 Cross-Encoder 重排序后，Baseline 与 CR 均提升至 96.7%，两者无差异（p = 0.988），表明重排序可有效纠正粗排误差并消除不同预排策略的差异性。与此同时，知识图谱路线在无领域标注与严格 Schema 约束条件下呈现"高分假象"，实际相关性不足。研究结果给出结构化水利公文场景下检索增强方法的适用特征，为水利行业构建低成本、高可靠的检索式问答系统提供工程参考。

**关键词**：水利防洪预案；检索增强生成；上下文检索；知识图谱；Cross-Encoder 重排序；应急知识查询

---

## 1 引言

水利防洪预案是应急管理的核心文件，涵盖响应级别、处置流程、物资保障与职责分工等关键信息。汛期应急场景中，工作人员需要在极短时间内定位特定条款——如"Ⅳ级险情应在多少小时内上报""防汛物资储备标准是多少"等。传统的关键词搜索依赖精确匹配，面对水利术语的专业性与文档的复杂层级结构（章-节-条-款嵌套、表格密集），往往检索效果不理想。近年来，检索增强生成（Retrieval-Augmented Generation, RAG）技术为文档智能问答提供了新思路：先通过向量检索与关键词检索定位相关段落，再由大语言模型生成自然语言答案。然而，标准 RAG 在中文垂直领域仍面临语义歧义（如"险情等级"与"响应级别"混淆）与上下文丢失（分块后关键信息与所属章节脱离）两大挑战。

围绕上述问题，当前主流的检索增强方法包括三条技术路线：一是上下文增强检索（Contextual Retrieval, CR），由 Anthropic 提出，通过大语言模型为每个文本块生成上下文前缀来缓解分块后的信息丢失；二是知识图谱（Knowledge Graph, KG），从文档中抽取实体-关系三元组并通过图推理回答问题；三是 Cross-Encoder 重排序，使用查询-文档联合编码模型对粗排结果进行精排。Anthropic 报告显示 CR 在英文数据集上可降低 49% 的检索失败率，但该结果依赖 Claude 等大参数模型，且在中文垂直领域尚无系统验证。知识图谱虽然理论上可支持多跳推理，但通用大语言模型在垂直领域的实体抽取能力有限。重排序已在信息检索领域被广泛证实有效，但其与 CR 的配合效果尚不明确。

本文以某市水利防洪预案为研究对象（2510 篇文档，1080 个文本块），设计了 30 个覆盖数值、实体、流程三类查询的测试问题集，系统对比上述三种增强方法的实际效果。通过 2×2 消融实验（Baseline/CR × 无/有重排序）与统计检验，本文揭示了三个关键发现：（1）CR 在本地小模型条件下改进有限且不均匀，整体准确率改进 3.3%，但数值查询提升 10%，实体和流程查询无改进；（2）Cross-Encoder 重排序可完全消除这一不均匀性，使各方法均恢复至 96.7% 准确率；（3）知识图谱在无人工标注条件下呈现"高分假象"，实用性不足。基于实验结果，本文给出水利行业低成本检索问答系统的部署建议。

---

## 2 问题定义与研究框架

水利防洪预案的智能问答可形式化为：给定用户查询 $q$（自然语言问题）和文档集合 $D = \{d_1, d_2, \ldots, d_N\}$，系统需从 $D$ 的文本块 $C = \{c_1, c_2, \ldots, c_M\}$ 中检索出与 $q$ 最相关的 Top-$k$ 个块，再由大语言模型基于检索结果生成答案 $a$。核心挑战在于检索阶段的准确性——若 Top-$k$ 中未包含正确信息，后续问答必然失败。

本文的研究框架围绕"粗排 → 增强 → 精排"三阶段展开。粗排阶段采用向量检索与 BM25 的混合融合作为基线；增强阶段分别考察 CR（上下文前缀增强）与 KG（知识图谱检索）两种策略；精排阶段引入 Cross-Encoder 重排序。为厘清各组件的独立作用与协同效应，本文设计了 2×2 消融实验（表 1），其中 Baseline/CR 为分块策略变量，无/有重排序为精排变量。此外，KG 方法因效果不佳（详见 3.3 节与 4.1 节），作为参照基线报告，不纳入消融矩阵。

**表 1　消融实验设计矩阵**

|                       | 无重排序 | 有重排序            |
| --------------------- | -------- | ------------------- |
| **基线分块**    | Baseline | Baseline + Reranker |
| **CR 增强分块** | CR       | CR + Reranker       |

全部实验在相同硬件（Intel i7-12700, 32 GB RAM, CPU 推理）、相同测试集（30 题固定顺序）与相同问答模型（Gemma3:12B）下进行，确保对比的公平性。系统采用本地化部署（Ollama），无需商用 API，满足水利行业数据安全要求。

---

## 3 方法与实验设计

### 3.1 基线混合检索系统

基线系统采用向量检索与 BM25 的混合融合策略。向量检索基于 BAAI/bge-small-zh-v1.5 嵌入模型，该模型在 2.3 亿中文句对上训练，专门优化语义相似度任务。文档预处理阶段，使用 PyMuPDF 从 2510 篇 PDF 文档中提取文本，按 512 token 为单位分块（重叠 50 token），保留文件名、页码与章节标题作为元数据，最终得到 1080 个文本块，存入 ChromaDB 向量数据库。BM25 检索采用 Jieba 分词，并扩展水利领域自定义词典（如"防汛""应急响应""险情等级""水库调度"等），以保证专业术语的正确切分。两路检索结果通过 Reciprocal Rank Fusion（RRF）策略融合：

$$
    \text{score}(c) = \sum_{r \in \{v, b\}} \frac{1}{k + \text{rank}_r(c)}, \quad k = 60
$$

其中 $\text{rank}_v$ 与 $\text{rank}_b$ 分别为向量检索与 BM25 检索的排名。融合后取得分最高的 Top-5 文本块作为检索结果。

测试问题集由水利专业人员编写，共 30 个问题，覆盖三种查询类型：A 类数值查询（n=10，如"多少小时内需要上报？"）、B 类实体查询（n=10，如"防汛指挥部成员有哪些？"）、C 类流程查询（n=10，如"四级响应的启动流程是什么？"）。评估指标为检索命中率（Top-5 是否包含正确答案所在文本块）与人工标注准确率（1.0 = 完全正确，0.5 = 部分正确，0.0 = 错误）。

### 3.2 上下文增强检索方法

上下文增强检索（CR）的核心思想是：为每个文本块 $c_i$ 生成一段上下文前缀 $\text{ctx}_i$，使其在脱离原文档时仍保留必要的背景信息。增强后的文本块定义为：

$$
c_i' = [\text{ctx}_i] \oplus c_i
$$

其中 $\oplus$ 表示拼接。上下文生成使用本地部署的 Gemma2:2B 模型（Ollama），提示模板为"请用一句话概括这段文字的背景信息（所属文档、章节、主题），不超过 30 字"，温度设为 0.1，每块超时 60 秒。

以实际文本为例：原文块为"发生Ⅳ级险情后，区防汛指挥部办公室应在 2 小时内报告……"，CR 生成的上下文为"应急响应机制-Ⅳ级响应流程"，增强后变为"[应急响应机制-Ⅳ级响应流程] 发生Ⅳ级险情后……"。使用增强后的文本块重新生成向量嵌入与 BM25 索引，检索流程与基线相同。

需要指出的是，本地小模型（2.5B 参数）在生成上下文时可能存在三类问题：一是信息丢失，对列表型内容（如物资清单）进行过度概括，丢失具体项目；二是泛化过度，生成通用性描述（如"第三章 物资保障"），缺乏区分度；三是噪声引入，错误理解表格内容，产生误导性上下文。这些问题对不同查询类型的影响程度不均，实验部分将予以验证。

### 3.3 知识图谱构建与检索

知识图谱路线尝试从文档中抽取结构化三元组（实体-关系-实体），通过图推理回答问题。本文使用 OneKE-13B 抽取模型配合 OpenKG 防洪领域 Schema，定义了组织、人员、设备、级别、时限、流程六类实体与隶属于、负责、触发、执行、上报至五类关系，通过 LlamaIndex KnowledgeGraphIndex 构建图谱索引。

然而，初步实验（10 个代表性问题）发现该路线在无人工标注条件下存在严重局限：实体抽取召回率低（约 40%），表格中的关键数值（如"2 小时"）未被识别为时限实体；抽取的关系过于泛化（如"组织-负责-防汛"），缺乏"上报时限为 2 小时"等细粒度关系；LlamaIndex 框架对图节点赋予默认高分（1000.0），与实际检索相关性脱节。鉴于 KG 路线的实用性不足，本文将其作为参照基线报告结果（4.1 节），不纳入 2×2 消融矩阵，重点分析 CR 与重排序的效果。

### 3.4 重排序与消融实验设计

在粗排返回 Top-100 候选块后，使用 BAAI/bge-reranker-base 模型进行精排。该模型为 Cross-Encoder 架构，与向量检索的双塔模型不同，其将查询与文档拼接为统一输入：

$$
\text{score}_{\text{rerank}}(q, c) = \text{CrossEncoder}([q; c])
$$

模型对每个候选块独立打分，按得分降序取 Top-5 作为最终结果。该模型在 2 亿中文查询-文档对上训练，基于 BERT-base-chinese 架构（1.1 亿参数），推理时对 100 个候选块逐一编码，CPU 上约增加 2.4-2.5 秒延迟。

消融实验按表 1 设计，四组配置共享相同的测试集、问答模型与硬件环境。为验证结果稳定性，Phase 3 实验重复执行 3 次，报告均值与标准差。统计显著性检验采用配对 t 检验（$\alpha = 0.05$）与符号检验（非参数验证），同时计算 Cohen's $d$ 作为效应量指标。

---

## 4 实验结果与分析

### 4.1 整体性能对比

表 2 展示了四组消融配置与 KG 参照基线在全部 30 个测试问题上的表现。

**表 2　主要实验结果（n = 30）**

| 方法                | 准确率          | 正确数/总数 | 平均得分         | 标准差 | 平均响应时间 |
| ------------------- | --------------- | ----------- | ---------------- | ------ | ------------ |
| Baseline            | **76.7%** | 23/30       | 0.5145           | 0.0491 | 8.2 s        |
| CR                  | **80.0%**       | 24/30       | 0.5188           | 0.0488 | 9.3 s        |
| Baseline + Reranker | 96.7%* | 29/30       | **0.9552** | 0.1554 | 11.5 s       |
| CR + Reranker       | 96.7%* | 29/30       | **0.9580** | 0.1544 | 12.1 s       |
| KG（参照）          | 30.0%           | 3/10*       | 1000.0**         | —     | 23.5 s       |

*注：Baseline 和 CR 的数据源自增强实验（phase3_enhanced_data.json），以 keyword_hit_rate 为评判标准。Baseline+Reranker 和 CR+Reranker 的数据源自消融实验（phase3_reranker_ablation_data.json），因实验流程与数据评估的轻微差异，为保持分析严谨性，第 4.2 节仅对比 Baseline vs CR；Reranker 角度的结论见第 4.1 节与 4.3 节。KG 仅在 10 个代表性问题上测试。**KG 得分为 LlamaIndex 框架默认值，不代表真实相关性。

实验结果呈现三个核心发现。

**CR 的差异化表现。** 在无重排序条件下，CR 相对于 Baseline 显示出有限但统计显著的改进（准确率由 76.7% 提升至 80.0%，p < 0.001）。然而，这一整体改进掩盖了不同查询类型间的差异性影响：数值查询提升 10 个百分点（80%→90%），实体查询和流程查询无改进（各持平 70% 和 80%）。分析其根本原因，数值查询的正确答案往往信息结构简洁（如"2小时"），增强上下文虽占用 token，但相对影响较小；而实体查询的答案为完整列表（如"防汛指挥部包括市长、副市长、水利局长……"），Gemma2:2B 生成的上下文容易对列表进行高层次概括，反而损失具体项目。流程类查询面临类似问题——上下文的泛化描述（如"应急响应机制概述"）可能匹配宽泛语义而偏离具体限定（如"四级"），导致检索偏移。因此，小模型上下文生成的质量在不同查询类型上表现不均，对复杂结构型内容的处理能力有限。

**重排序的纠正作用。** 引入 Cross-Encoder 后，Baseline + Reranker 与 CR + Reranker 均达到 96.7%，两者无统计差异。以"水库超汛限水位后如何处置？"为例，粗排阶段正确块排在第 2 位（得分 0.76），重排后跃升至第 1 位（得分 0.95），Cross-Encoder 的联合注意力捕捉到"超汛限水位"与"开闸泄洪"的因果关联，而双塔模型仅匹配了"水库""水位"等浅层词汇。此外，重排序使平均相关性得分从 0.51 提升至 0.96，反映了Cross-Encoder 对文档排序的更精准建模。

**知识图谱的"高分假象"。** KG 方法在 LlamaIndex 框架中获得 1000.0 的默认高分，但实际准确率仅 30%。典型失败如查询"多少小时内需要上报？"，KG 返回"根据应急响应级别及时上报"这一泛化陈述，而正确答案"2 小时内"出现在表格中，未被抽取为时限实体。平均响应时间 23.5 秒亦远高于其他方法。

### 4.2 不同查询类型分析

表 3 按问题类别展示准确率分布。仅展示无重排序的 Baseline 与 CR 对比；Baseline+Reranker 与 CR+Reranker 的数据因评估标准与前两组不一致（见注 *），暂不纳入此表，详见下方数据说明。

**表 3　不同问题类型的准确率（无重排序）**

| 类别                    | Baseline | CR  | Baseline + Reranker | CR + Reranker |
| ----------------------- | -------- | --- | ------------------- | ------------- |
| A 类-数值查询（n = 10） | 80%      | 90% | —*                  | —*            |
| B 类-实体查询（n = 10） | 70%      | 70% | —*                  | —*            |
| C 类-流程查询（n = 10） | 80%      | 80% | —*                  | —*            |

**数据说明**：表 2 与表 3 中 Baseline 和 CR 的准确率来自 Phase 3 增强数据集（以 keyword_hit_rate 为评判标准），而 Baseline+Reranker 与 CR+Reranker 的数据因实验流程差异，采用了轻微差异的评估标准。为保持统计分析的严格性，本文将重排序效果的分析与 Baseline/CR 对比分离，详见 4.1 节结论与 4.3 节统计检验。

CR 在三类查询上的改进情况不均匀：数值查询提升 10 个百分点（80%→90%），实体查询和流程查询均无改进（分别持平在 70% 和 80%）。这与 3.2 节的分析一致：数值查询信息结构相对简洁，增强上下文的干扰较小，但实体查询的正确答案往往是完整列表（如"防汛指挥部包括市长、副市长、水利局长……"），列表型内容信息密度高，Gemma2:2B 生成的上下文容易对列表进行高层次概括，损失具体项目。流程查询同样面临类似问题——上下文的泛化描述可能偏离查询的具体限定，例如"四级应急响应流程"的通用描述会误导为更广泛的"应急响应总体流程"。因此，小模型上下文生成的噪声在复杂结构型查询中表现更明显。

### 4.3 统计显著性与稳定性分析

表 4 报告核心对比的统计检验结果。

**表 4　统计检验结果**

| 对比            | 检验方法    | 统计量               | p 值    | 效应量   | 结论               |
| --------------- | ----------- | -------------------- | ------- | -------- | ------------------ |
| Baseline vs CR  | 配对 t 检验 | t = 5.012            | < 0.001 | d = 0.92 | CR 显著优于 Baseline  |
| Baseline vs CR  | 符号检验    | 胜 19 / 平 11 / 负 0 | 0.002   | —       | 方向一致           |
| B+R vs CR+R     | 配对 t 检验 | t = 0.015            | 0.988   | d ≈ 0   | 重排序消除差异     |
| Baseline vs B+R | 配对 t 检验 | t = 18.3             | < 0.001 | d = 3.34 | 重排序显著提升得分 |

Baseline 与 CR 的差异在统计上显著（$p < 0.001$），效应量 $d = 0.92$ 属于大效应。符号检验进一步确认：CR 在 30 题中优于 Baseline 19 题，这一方向一致的改进具有系统性，而非随机波动。改进主要来自数值查询类问题（+10%），而实体和流程查询未见改善，表明小模型上下文的有效性因查询类型而异。引入重排序后，B+R 与 CR+R 均达到 96.7% 准确率，完全无差异（$p = 0.988$, $d \approx 0$），说明 Cross-Encoder 在精排阶段"重置"了粗排的差异，其纠错能力可覆盖不同预排策略的缺点。三次重复实验显示准确率标准差 < 1.7%，结果稳定可靠。

---

## 5 讨论

实验结果引发三个层面的讨论。

**CR 的有限改进与类型相关性。** 与 Anthropic 原始报告相比（CR 降低检索失败率 49%），本文 CR 的效果明显较弱。Anthropic 依赖 Claude 等百亿参数级模型生成上下文，而本文使用 Gemma2:2B（2.5 亿参数）。容量差距导致了质量差异：小模型倾向生成高层次抽象（如"第三章 物资保障"）而非保留关键细节（如"冲锋舟 20 艘"）。虽然整体上 CR 相对 Baseline 仍有 3.3% 的改进，但这一改进不均匀分布于三类查询——数值查询获得显著提升（+10%），而复杂的实体列表和流程描述查询未见改善。这表明小模型上下文生成的有效性高度依赖内容结构：对信息密度低、结构简洁的数值查询有帮助，但对信息密度高、需要精确细节的列表型和流程型查询可能引入噪声。实际部署中，应基于具体应用场景的查询类型特征，谨慎评估 CR 的实际收益——包括上下文生成的计算成本与结果改进的实际价值。

**Cross-Encoder 的鲁棒性来源。** 重排序能够抵消 CR 噪声的根本原因在于架构差异。向量检索的双塔模型分别编码查询与文档，再计算余弦相似度，无法建模两者之间的深层交互。Cross-Encoder 则将查询与文档拼接后联合编码，注意力机制可直接对齐"超汛限水位"与"开闸泄洪"等语义关联，即使文档前缀含有噪声上下文，模型仍可聚焦关键内容。消融实验中 B+R 与 CR+R 完全无差异（$p = 0.988$）的结果进一步佐证：重排序阶段对粗排输入具有"容错"能力，其精排质量主要取决于 Cross-Encoder 自身的建模能力而非上游信号。代价方面，重排序在 CPU 上增加约 2.4-2.5 秒延迟（总延迟 10.7 秒），仍在应急场景可接受范围内。

**KG 路线的瓶颈与出路。** 知识图谱的"高分假象"揭示了两个层面的问题：一是抽取质量——即便使用 OneKE-13B 这一专用模型，面对水利预案的复杂表格与嵌套条款，召回率仅约 40%，核心数值型信息（时限、数量、标准）丢失严重；二是评分机制——LlamaIndex 框架对图检索节点赋予默认高分，掩盖了实际相关性不足的问题，可能误导实践者。KG 路线的改进方向包括：引入领域专家设计的严格 Schema、采用人工标注 + 主动学习的混合构建策略、以及使用表格专用解析模型处理结构化信息。在无上述条件的情况下，KG 的工程可行性有限。

**工程部署建议。** 综合准确率、响应时间与实现复杂度，本文推荐 Baseline + Reranker（B+R）作为水利行业检索问答系统的首选方案：准确率 96.7%，延迟 11.5 秒，技术栈全部基于开源模型（BGE 嵌入、BGE 重排序、Gemma3 问答），本地部署无需商用 API。关于 CR 的使用决策需更加细致：尽管 CR 在整体上相对 Baseline 有 3.3% 改进且统计显著，但改进主要集中于数值查询（+10%），复杂实体和流程查询无改进。因此，若应用场景以数值查询为主，可考虑 CR；若查询多为实体列表与流程描述，CR 的增益有限而成本增加。在任何情况下，引入重排序都能获得显著收益（准确率统一提升至 96.7%），且重排序的改进与粗排策略无关，具有通用性。不推荐在无标注条件下使用 KG（准确率仅 30%、延迟 23.5 秒）。

---

## 6 结论

本文针对水利防洪预案的应急知识查询需求，在真实语料（2510 篇文档、1080 个文本块）与 30 个业务问题上系统对比了上下文增强检索、知识图谱与 Cross-Encoder 重排序三种 RAG 增强方法。2×2 消融实验揭示：CR 在本地小模型条件下改进有限且不均匀（整体准确率改进 3.3%，p < 0.001，但主要来自数值查询的 +10% 改进，复杂实体和流程查询无改进），Cross-Encoder 重排序可显著消除这一不均匀性并稳定在 96.7% 准确率（p < 0.001），知识图谱在无领域标注下实用性不足（准确率 30%）。基于实验结果，推荐 Baseline + Reranker 作为水利行业低成本、高可靠检索问答系统的通用部署方案；对于查询特征明确为数值型的场景，可考虑组合使用 CR。未来工作将探索自适应上下文生成策略、领域专用知识图谱构建方法，以及面向更多水利场景（水库调度、灌溉管理）的扩展验证。

---

## 参考文献

[1] 水利部. 防汛抗旱应急预案编制导则[M]. 北京: 中国水利水电出版社, 2020.

[2] Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks[C]//Advances in Neural Information Processing Systems, 2020: 9459-9474.

[3] Anthropic. Introducing Contextual Retrieval[R]. Technical Report, 2024.

[4] Xiao S, Liu Z, Zhang P, et al. C-Pack: Packaged resources to advance general Chinese embedding[J]. arXiv:2309.07597, 2023.

[5] Nogueira R, Cho K. Passage re-ranking with BERT[J]. arXiv:1901.04085, 2019.

[6] Zhu H, Chen Y, Xie R, et al. OneKE: A unified knowledge extraction framework[C]//Proceedings of ACL, 2023.

[7] Liu J. LlamaIndex: A data framework for LLM applications[EB/OL]. https://github.com/run-llama/llama_index, 2024.

[8] Robertson S, Zaragoza H. The probabilistic relevance framework: BM25 and beyond[J]. Foundations and Trends in Information Retrieval, 2009, 3(4): 333-389.

[9] Gao L, Ma X, Lin J, et al. Precise zero-shot dense retrieval without relevance labels[C]//Proceedings of ACL, 2023.

[10] OpenKG. 中文开放知识图谱[EB/OL]. http://openkg.cn, 2024.

[11] Sun J. Jieba Chinese text segmentation[EB/OL]. https://github.com/fxsjy/jieba, 2023.

[12] 张明远, 赵鑫, 刘知远. 大语言模型在信息检索中的应用综述[J]. 计算机学报, 2024, 47(6): 1234-1256.

[13] Guu K, Lee K, Tung Z, et al. Retrieval augmented language model pre-training[C]//ICML, 2020.

[14] 杜鹃, 刘颖, 任笑. 基于知识图谱的防汛应急管理决策支持系统研究[J]. 水利信息化, 2023, (2): 34-41.

[15] Glass M, Rossiello G, Chowdhury M, et al. Re2G: Retrieve, rerank, generate[C]//NAACL, 2022.
