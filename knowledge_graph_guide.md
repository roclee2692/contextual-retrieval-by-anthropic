# é¾™å­æ¹–é£Ÿå ‚çŸ¥è¯†å›¾è°±æ„å»ºæŒ‡å—

## ğŸ¯ æ–¹æ¡ˆï¼šOneKE + LlamaIndex çŸ¥è¯†å›¾è°± RAG

### æŠ€æœ¯æ ˆ
- **OneKE**: çŸ¥è¯†æŠ½å–ï¼ˆæå–ä¸‰å…ƒç»„ï¼‰
- **LlamaIndex**: å›¾è°±ç´¢å¼•å’ŒæŸ¥è¯¢
- **Neo4j/NetworkX**: å›¾æ•°æ®åº“
- **Ollama**: LLM æ¨ç†

---

## ğŸ“Š æ¶æ„è®¾è®¡

```
PDF æ–‡æ¡£
    â†“
OneKE æŠ½å–
    â†“
ä¸‰å…ƒç»„ (å®ä½“-å…³ç³»-å®ä½“)
    â†“
LlamaIndex KnowledgeGraphIndex
    â†“
å›¾è°±æ¨ç† + å‘é‡æ£€ç´¢
    â†“
LLM ç”Ÿæˆç­”æ¡ˆ
```

---

## ğŸ”§ å®ç°æ­¥éª¤

### æ­¥éª¤ 1: å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–ï¼ˆå·²æœ‰ï¼‰
pip install llama-index

# çŸ¥è¯†å›¾è°±ç›¸å…³
pip install llama-index-graph-stores-neo4j  # å¦‚æœç”¨ Neo4j
pip install networkx matplotlib  # å¦‚æœç”¨ NetworkXï¼ˆè½»é‡çº§ï¼‰

# OneKE æŠ½å–å·¥å…·
pip install oneke  # æˆ–ä½¿ç”¨ API ç‰ˆæœ¬
```

### æ­¥éª¤ 2: ä½¿ç”¨ OneKE æå–ä¸‰å…ƒç»„

**æ–¹å¼ A: ä½¿ç”¨ OneKE Python åº“**
```python
from oneke import OneKE

# åˆå§‹åŒ– OneKE
extractor = OneKE(model="oneke-v1")

# è¯»å– PDF æ–‡æœ¬
with open("./data/NCWU_Longzihu_Canteens_RAG_Chunked.pdf", "r") as f:
    text = f.read()

# æå–ä¸‰å…ƒç»„
triples = extractor.extract(text)
# è¾“å‡ºæ ¼å¼: [(head, relation, tail), ...]
# ä¾‹å¦‚: [("ä¸€å·é¤å…", "æœ‰çª—å£", "19å·æˆ‘çˆ±æˆ‘ç²¥"), 
#        ("æˆ‘çˆ±æˆ‘ç²¥", "æä¾›", "å°ç±³å—ç“œç²¥"), 
#        ("å°ç±³å—ç“œç²¥", "ä»·æ ¼", "2å…ƒ")]
```

**æ–¹å¼ B: ä½¿ç”¨ LlamaIndex å†…ç½®æŠ½å–**
```python
from llama_index.core import KnowledgeGraphIndex
from llama_index.core import SimpleDirectoryReader
from llama_index.llms.ollama import Ollama

# è¯»å–æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# ä½¿ç”¨ Ollama æŠ½å–çŸ¥è¯†å›¾è°±
llm = Ollama(model="gemma3:12b", base_url="http://localhost:11434")

# è‡ªåŠ¨æŠ½å–ä¸‰å…ƒç»„
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    llm=llm,
    max_triplets_per_chunk=10,
    include_embeddings=True
)
```

### æ­¥éª¤ 3: åˆ›å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•

**å®Œæ•´ä»£ç ç¤ºä¾‹**:

```python
from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader
from llama_index.core.graph_stores import SimpleGraphStore
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import os

# é…ç½®
os.environ["OLLAMA_BASE_URL"] = "http://localhost:11434"

# åˆå§‹åŒ– LLM
llm = Ollama(
    model="gemma3:12b",
    base_url=os.getenv("OLLAMA_BASE_URL"),
    request_timeout=120.0
)

# åˆå§‹åŒ– Embedding
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-base-en-v1.5"
)

# è¯»å–æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆ›å»ºå›¾å­˜å‚¨
graph_store = SimpleGraphStore()

# åˆ›å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•
kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    llm=llm,
    embed_model=embed_model,
    graph_store=graph_store,
    max_triplets_per_chunk=5,
    include_embeddings=True,
    show_progress=True
)

# ä¿å­˜å›¾è°±
kg_index.storage_context.persist(persist_dir="./src/db/knowledge_graph")
```

### æ­¥éª¤ 4: æŸ¥è¯¢çŸ¥è¯†å›¾è°±

```python
# åŠ è½½å·²ä¿å­˜çš„å›¾è°±
from llama_index.core import load_index_from_storage
from llama_index.core import StorageContext

storage_context = StorageContext.from_defaults(
    persist_dir="./src/db/knowledge_graph"
)
kg_index = load_index_from_storage(storage_context)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = kg_index.as_query_engine(
    include_text=True,  # åŒ…å«åŸå§‹æ–‡æœ¬
    response_mode="tree_summarize",
    embedding_mode="hybrid",
    similarity_top_k=5
)

# æŸ¥è¯¢
response = query_engine.query("ä¸€å·é¤å…æœ‰å“ªäº›çª—å£ï¼Ÿ")
print(response)

# è·å–å­å›¾ï¼ˆå¯è§†åŒ–ç›¸å…³å®ä½“ï¼‰
sub_graph = kg_index.get_networkx_graph()
```

---

## ğŸ¨ çŸ¥è¯†å›¾è°±ç»“æ„ç¤ºä¾‹

### é£Ÿå ‚é¢†åŸŸä¸‰å…ƒç»„
```
(ä¸€å·é¤å…, ç±»å‹, é£Ÿå ‚)
(ä¸€å·é¤å…, ä½ç½®, é¾™å­æ¹–æ ¡åŒº)
(ä¸€å·é¤å…, åŒ…å«çª—å£, 19å·æˆ‘çˆ±æˆ‘ç²¥)
(19å·æˆ‘çˆ±æˆ‘ç²¥, æä¾›, å°ç±³å—ç“œç²¥)
(å°ç±³å—ç“œç²¥, ä»·æ ¼, 2å…ƒ)
(å°ç±³å—ç“œç²¥, å®¹é‡, ä¸€æ¯)

(äºŒå·é¤å…, ç±»å‹, é£Ÿå ‚)
(äºŒå·é¤å…, æ¥¼å±‚, ä¸€æ¥¼)
(äºŒå·é¤å…, åŒ…å«çª—å£, 21å·å¤©æ´¥åŒ…å­)
(21å·å¤©æ´¥åŒ…å­, æä¾›, æ‹›ç‰Œé²œè‚‰åŒ…)
(æ‹›ç‰Œé²œè‚‰åŒ…, ä»·æ ¼, 2å…ƒ)
```

### å›¾è°±ä¼˜åŠ¿
1. **å…³ç³»æ¨ç†**: "ä¸€å·é¤å…æœ‰å“ªäº›2å…ƒçš„é£Ÿå“ï¼Ÿ" â†’ éå†ä»·æ ¼å…³ç³»
2. **å¤šè·³æŸ¥è¯¢**: "æœ€ä¾¿å®œçš„åŒ…å­åœ¨å“ªä¸ªé£Ÿå ‚ï¼Ÿ" â†’ ä»·æ ¼æ¯”è¾ƒ + çª—å£å½’å±
3. **å®ä½“èšåˆ**: "æ‰€æœ‰ç²¥ç±»äº§å“" â†’ æŒ‰ç±»åˆ«èšåˆ

---

## ğŸ”„ æ··åˆæ£€ç´¢ï¼šå‘é‡ + å›¾è°± + BM25

```python
from llama_index.core import QueryBundle
from llama_index.core.retrievers import (
    VectorIndexRetriever,
    KnowledgeGraphRAGRetriever,
    BM25Retriever
)
from llama_index.core.query_engine import RetrieverQueryEngine

# 1. å‘é‡æ£€ç´¢å™¨
vector_retriever = VectorIndexRetriever(
    index=vector_index,
    similarity_top_k=3
)

# 2. çŸ¥è¯†å›¾è°±æ£€ç´¢å™¨
kg_retriever = KnowledgeGraphRAGRetriever(
    storage_context=kg_storage_context,
    graph_store=graph_store,
    llm=llm,
    include_text=True
)

# 3. BM25 æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_defaults(
    nodes=nodes,
    similarity_top_k=3
)

# èåˆæŸ¥è¯¢å¼•æ“
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector

query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(llm=llm),
    query_engine_tools=[
        vector_tool,
        kg_tool,
        bm25_tool
    ]
)

response = query_engine.query("ä¸€å·é¤å…æœ‰ä»€ä¹ˆä¾¿å®œçš„æ—©é¤ï¼Ÿ")
```

---

## ğŸ“ å®Œæ•´è„šæœ¬ï¼šcreate_knowledge_graph.py

```python
"""
åˆ›å»ºé£Ÿå ‚çŸ¥è¯†å›¾è°±
"""
import os
from dotenv import load_dotenv
from llama_index.core import (
    KnowledgeGraphIndex,
    SimpleDirectoryReader,
    StorageContext
)
from llama_index.core.graph_stores import SimpleGraphStore
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

load_dotenv()

print("="*80)
print("é¾™å­æ¹–é£Ÿå ‚çŸ¥è¯†å›¾è°±æ„å»º")
print("="*80)

# é…ç½®
DATA_DIR = os.getenv("DATA_DIR", "./data")
SAVE_DIR = "./src/db/knowledge_graph"
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# åˆå§‹åŒ–ç»„ä»¶
print("\n[1/4] åˆå§‹åŒ– LLM å’Œ Embedding...")
llm = Ollama(
    model="gemma3:12b",
    base_url=OLLAMA_BASE_URL,
    request_timeout=180.0
)

embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-base-en-v1.5"
)

# è¯»å–æ–‡æ¡£
print("\n[2/4] è¯»å– PDF æ–‡æ¡£...")
documents = SimpleDirectoryReader(DATA_DIR).load_data()
print(f"âœ“ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# åˆ›å»ºå›¾å­˜å‚¨
print("\n[3/4] åˆ›å»ºçŸ¥è¯†å›¾è°±ï¼ˆå¯èƒ½éœ€è¦ 10-30 åˆ†é’Ÿï¼‰...")
graph_store = SimpleGraphStore()

kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    llm=llm,
    embed_model=embed_model,
    graph_store=graph_store,
    max_triplets_per_chunk=5,
    include_embeddings=True,
    show_progress=True
)

# ä¿å­˜å›¾è°±
print("\n[4/4] ä¿å­˜çŸ¥è¯†å›¾è°±...")
kg_index.storage_context.persist(persist_dir=SAVE_DIR)

print("\n" + "="*80)
print("âœ… çŸ¥è¯†å›¾è°±åˆ›å»ºå®Œæˆï¼")
print(f"ä¿å­˜ä½ç½®: {SAVE_DIR}")
print("="*80)

# æµ‹è¯•æŸ¥è¯¢
print("\næµ‹è¯•æŸ¥è¯¢: é¾™å­æ¹–æ ¡åŒºæœ‰å‡ ä¸ªé£Ÿå ‚ï¼Ÿ")
query_engine = kg_index.as_query_engine(
    include_text=True,
    response_mode="tree_summarize"
)
response = query_engine.query("é¾™å­æ¹–æ ¡åŒºæœ‰å‡ ä¸ªé£Ÿå ‚ï¼Ÿ")
print("\nç­”æ¡ˆ:")
print(response)
```

---

## ğŸ¯ å¯¹æ¯”ä¸‰ç§æ–¹æ³•

| æ–¹æ³• | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **çº¯å‘é‡æ£€ç´¢** | å¿«é€Ÿã€è¯­ä¹‰ç†è§£å¥½ | ç¼ºä¹ç»“æ„åŒ–æ¨ç† | å¼€æ”¾æ€§é—®é¢˜ |
| **BM25 å…³é”®è¯** | ç²¾ç¡®åŒ¹é…ã€å¿« | æ— è¯­ä¹‰ç†è§£ | å®ä½“æŸ¥æ‰¾ |
| **çŸ¥è¯†å›¾è°±** | å…³ç³»æ¨ç†ã€å¤šè·³æŸ¥è¯¢ | æ„å»ºæ…¢ã€éœ€è¦æŠ½å– | å¤æ‚å…³è”æŸ¥è¯¢ |

### æœ€ä½³å®è·µï¼šä¸‰è€…ç»“åˆ
```
ç”¨æˆ·æŸ¥è¯¢
    â†“
è·¯ç”±å™¨é€‰æ‹©
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‘é‡æ£€ç´¢ â”‚ BM25     â”‚ å›¾è°±æ¨ç† â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“         â†“         â†“
    ç»“æœèåˆ + é‡æ’åº
    â†“
    LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: è½»é‡çº§ï¼ˆNetworkXï¼‰
```bash
pip install networkx matplotlib
python create_knowledge_graph.py
```

### æ–¹å¼ 2: ç”Ÿäº§çº§ï¼ˆNeo4jï¼‰
```bash
# å¯åŠ¨ Neo4j Docker
docker run -d \
  --name neo4j \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/password \
  neo4j:latest

# å®‰è£…ä¾èµ–
pip install llama-index-graph-stores-neo4j

# ä¿®æ”¹ä»£ç ä½¿ç”¨ Neo4jGraphStore
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æŸ¥è¯¢ç¤ºä¾‹
**Q**: "ä¸€å·é¤å…æœ‰å“ªäº›2å…ƒçš„é£Ÿå“ï¼Ÿ"

**å›¾è°±æ¨ç†**:
1. æ‰¾åˆ°å®ä½“ "ä¸€å·é¤å…"
2. éå† "åŒ…å«çª—å£" å…³ç³»
3. éå† "æä¾›" å…³ç³»
4. è¿‡æ»¤ "ä»·æ ¼=2å…ƒ"
5. è¿”å›æ‰€æœ‰ç¬¦åˆçš„é£Ÿå“

**ç­”æ¡ˆ**: 
- å°ç±³å—ç“œç²¥ï¼ˆ19å·æˆ‘çˆ±æˆ‘ç²¥ï¼‰
- æ¸…ç«ç»¿è±†ç²¥ï¼ˆ19å·æˆ‘çˆ±æˆ‘ç²¥ï¼‰
- ...

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æŠ½å–è´¨é‡**: OneKE/LLM æŠ½å–çš„ä¸‰å…ƒç»„éœ€è¦äººå·¥å®¡æ ¸
2. **è®¡ç®—æˆæœ¬**: çŸ¥è¯†å›¾è°±æ„å»ºæ¯”çº¯å‘é‡æ£€ç´¢æ…¢ 5-10 å€
3. **å­˜å‚¨éœ€æ±‚**: å›¾è°±éœ€è¦é¢å¤–å­˜å‚¨ç©ºé—´
4. **æŸ¥è¯¢å¤æ‚åº¦**: å¤šè·³æŸ¥è¯¢å¯èƒ½è¾ƒæ…¢

---

## ğŸ“š å‚è€ƒèµ„æº

- [LlamaIndex Knowledge Graph](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/)
- [OneKE æ–‡æ¡£](https://github.com/zjunlp/DeepKE/tree/main/example/llm/OneKE)
- [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/)

---

**å»ºè®®**: å…ˆç”¨ç°æœ‰çš„å‘é‡+BM25æ–¹æ¡ˆå®Œæˆæµ‹è¯•ï¼ŒçŸ¥è¯†å›¾è°±ä½œä¸ºè¿›é˜¶ä¼˜åŒ–ï¼

