"""
å®Œæ•´è¯Šæ–­ï¼šæ£€æŸ¥æ•°æ®åº“ã€åˆ†è¯å™¨å’Œæ£€ç´¢è´¨é‡
"""
import chromadb
import jieba
from llama_index.retrievers.bm25 import BM25Retriever

print("=" * 80)
print("ğŸ” å®Œæ•´ç³»ç»Ÿè¯Šæ–­")
print("=" * 80)

# 1. æ£€æŸ¥å‘é‡æ•°æ®åº“
print("\nã€1ã€‘æ£€æŸ¥å‘é‡æ•°æ®åº“")
print("-" * 80)
try:
    client = chromadb.PersistentClient(path="./src/db/canteen_db_vectordb")
    collection = client.get_collection("ncwu_canteen_collection")
    
    results = collection.get()
    total_docs = len(results['ids'])
    print(f"âœ“ æ€»æ–‡æ¡£æ•°: {total_docs}")
    
    # æ£€æŸ¥å‰3ä¸ªå®Œæ•´æ–‡æ¡£
    sample = collection.get(limit=3, include=["documents", "metadatas"])
    for i, (doc, meta) in enumerate(zip(sample['documents'], sample['metadatas'])):
        print(f"\n--- æ–‡æ¡£ {i+1} ---")
        print(f"é•¿åº¦: {len(doc)} å­—ç¬¦")
        print(f"Metadata: {meta}")
        print(f"å‰200å­—ç¬¦:")
        print(doc[:200])
        
except Exception as e:
    print(f"âœ— å‘é‡æ•°æ®åº“é”™è¯¯: {e}")

# 2. æ£€æŸ¥BM25æ•°æ®åº“
print("\n\nã€2ã€‘æ£€æŸ¥BM25æ•°æ®åº“")
print("-" * 80)
try:
    bm25_retriever = BM25Retriever.from_persist_dir("./src/db/canteen_db_bm25")
    print(f"âœ“ BM25åŠ è½½æˆåŠŸ")
    
    # æµ‹è¯•æ£€ç´¢
    test_query = "åŒ…å­"
    results = bm25_retriever.retrieve(test_query)
    print(f"\næŸ¥è¯¢ '{test_query}' è¿”å› {len(results)} ä¸ªç»“æœ")
    
    for i, node in enumerate(results[:3]):
        print(f"\n--- BM25ç»“æœ {i+1} (åˆ†æ•°: {node.score:.4f}) ---")
        print(node.text[:150])
        
except Exception as e:
    print(f"âœ— BM25æ•°æ®åº“é”™è¯¯: {e}")

# 3. æµ‹è¯•åˆ†è¯å™¨
print("\n\nã€3ã€‘æµ‹è¯•åˆ†è¯å™¨")
print("-" * 80)

def chinese_tokenizer(text):
    """å¢å¼ºå‹ä¸­æ–‡åˆ†è¯å™¨"""
    tokens = list(jieba.cut_for_search(text))
    enhanced_tokens = []
    for token in tokens:
        enhanced_tokens.append(token)
        if 'åŒ…' in token:
            enhanced_tokens.append('åŒ…')
            enhanced_tokens.append('åŒ…å­')
    return enhanced_tokens

test_cases = [
    "å“ªäº›çª—å£æä¾›åŒ…å­ç±»é£Ÿå“ï¼Ÿ",
    "å¤©æ´¥åŒ…å­",
    "é¦™æ¸¯ä¹é¾™åŒ…",
    "æ¢…èœæ‰£è‚‰åŒ…",
    "é²œè‚‰åŒ…å­"
]

for text in test_cases:
    tokens = chinese_tokenizer(text)
    print(f"'{text}' â†’ {tokens}")

# 4. æ£€æŸ¥å…³é”®è¯è¦†ç›–
print("\n\nã€4ã€‘æ£€æŸ¥æ•°æ®åº“ä¸­çš„å…³é”®å®ä½“")
print("-" * 80)

keywords_to_check = [
    "ä¸€å·é¤å…", "äºŒå·é¤å…", "æ°‘æ—é¤å…",
    "åŒ…å­", "å¤©æ´¥åŒ…å­", "é¦™æ¸¯ä¹é¾™åŒ…",
    "çª—å£", "æ¡£å£", "42å·", "21å·"
]

for keyword in keywords_to_check:
    # åœ¨æ–‡æ¡£ä¸­æœç´¢
    found_count = 0
    for doc in sample['documents']:
        if keyword in doc:
            found_count += 1
    
    status = "âœ“" if found_count > 0 else "âœ—"
    print(f"{status} '{keyword}': å‡ºç°åœ¨ {found_count} ä¸ªæ ·æœ¬æ–‡æ¡£ä¸­")

print("\n\nã€5ã€‘æ¨èæ“ä½œ")
print("-" * 80)

# æ£€æŸ¥PDFæ–‡ä»¶
import os
pdf_path = "./data/"
if os.path.exists(pdf_path):
    pdfs = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]
    print(f"å½“å‰PDFæ–‡ä»¶: {pdfs}")
    
    if len(pdfs) > 1:
        print("\nâš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å¤šä¸ªPDFæ–‡ä»¶ï¼")
        print("   å»ºè®®: åªä¿ç•™ä¸€ä¸ªPDFï¼ˆCR_Prefixed_v2.pdfï¼‰")
    elif len(pdfs) == 1:
        print(f"\nâœ“ åªæœ‰ä¸€ä¸ªPDF: {pdfs[0]}")
    else:
        print("\nâœ— é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶ï¼")
else:
    print("âœ— data/ ç›®å½•ä¸å­˜åœ¨")

# æ£€æŸ¥æ–‡æ¡£è´¨é‡
if total_docs < 50:
    print("\nâš ï¸  è­¦å‘Š: æ–‡æ¡£æ•°é‡å¤ªå°‘ï¼")
    print(f"   å½“å‰: {total_docs} ä¸ªï¼Œå»ºè®® > 100")
    print("   æ“ä½œ: é‡æ–°è¿è¡Œ 'python create_save_db.py'")

print("\n" + "=" * 80)
print("è¯Šæ–­å®Œæˆ")
print("=" * 80)

