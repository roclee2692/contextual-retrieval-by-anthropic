# =====================================================
# 环境变量配置示例
# 使用方法: cp .env.example .env  然后按需修改
# =====================================================

# Ollama 服务地址（本地部署）
OLLAMA_BASE_URL="http://localhost:11434"

# LLM 模型名称（需要提前 ollama pull）
LLM_MODEL="gemma3:12b"

# 向量嵌入模型（HuggingFace，首次运行自动下载）
EMBEDDING_MODEL="BAAI/bge-small-zh-v1.5"

# 数据目录（防洪文档实验）
DATA_DIR="./data/防洪预案_txt"
SAVE_DIR="./src/db"
VECTOR_DB_PATH="./src/db/flood_prevention_db_vectordb"
BM25_DB_PATH="./src/db/flood_prevention_db_bm25"
COLLECTION_NAME="flood_prevention_collection"
DB_NAME="flood_prevention_db"

# 是否跳过上下文LLM增强（1=跳过，0=使用LLM增强）
SKIP_CONTEXT_LLM="0"

# API 地址（如果运行了 FastAPI 服务）
API_URL="http://127.0.0.1:8000/rag-chat"
